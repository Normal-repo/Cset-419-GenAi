{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12991771,"sourceType":"datasetVersion","datasetId":8223210},{"sourceId":632587,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":476870,"modelId":492807}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM,Dropout,BatchNormalization,Embedding,TimeDistributed,Activation\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-20T10:45:01.362429Z","iopub.execute_input":"2025-12-20T10:45:01.362702Z","iopub.status.idle":"2025-12-20T10:45:14.353255Z","shell.execute_reply.started":"2025-12-20T10:45:01.362679Z","shell.execute_reply":"2025-12-20T10:45:14.352471Z"}},"outputs":[{"name":"stderr","text":"2025-12-20 10:45:03.006525: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766227503.183139      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766227503.234398      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"sentences3 = []\n\nwith open(\"/kaggle/input/subtitles-dataset/en (1).txt\", \"r\", encoding=\"utf-8\") as f:\n    for i, line in enumerate(f):\n        if i >= 500000:   \n            break\n        line = line.strip()\n        if line:\n            sentences3.append(line.lower())\n\n\ntrain_sentences, val_sentences = train_test_split(sentences3, test_size=0.2, random_state=42)\nprint(\"Train sentences:\", len(train_sentences))\nprint(\"Validation sentences:\", len(val_sentences))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T10:45:14.354600Z","iopub.execute_input":"2025-12-20T10:45:14.355065Z","iopub.status.idle":"2025-12-20T10:45:14.904801Z","shell.execute_reply.started":"2025-12-20T10:45:14.355044Z","shell.execute_reply":"2025-12-20T10:45:14.904156Z"}},"outputs":[{"name":"stdout","text":"Train sentences: 400000\nValidation sentences: 100000\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"tokenizer_ng3 = Tokenizer(oov_token=\"<OOV>\")\ntokenizer_ng3.fit_on_texts(train_sentences)\nvocab_size_ng3 = len(tokenizer_ng3.word_index) + 1\nprint(\"Total vocab size:\", vocab_size_ng3)\n\n\ndef build_ngram_sequences_unique(lines, tokenizer, max_len=None):\n    seq_list = []\n    for line in lines:\n        t_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(t_list)):\n            seq_list.append(t_list[:i+1])\n\n    if not seq_list:\n        return None, None, max_len\n\n    if max_len is None:\n        max_len = max(len(s) for s in seq_list)\n\n    seq_list = pad_sequences(seq_list, maxlen=max_len, padding=\"pre\")\n    X_u, y_u = seq_list[:, :-1], seq_list[:, -1]\n    return X_u, y_u, max_len\n\n\n# Train set\nx0_ng, y0_ng, maxlen_ng3 = build_ngram_sequences_unique(train_sentences, tokenizer_ng3)\nprint(\"Train samples:\", x0_ng.shape, y0_ng.shape)\nprint(\"Max sequence length:\", maxlen_ng3)\n\n# Validation set\nxv_ng, yv_ng, _ = build_ngram_sequences_unique(val_sentences, tokenizer_ng3, max_len=maxlen_ng3)\n\nprint(\"Validation samples:\", xv_ng.shape, yv_ng.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T10:45:14.905547Z","iopub.execute_input":"2025-12-20T10:45:14.905811Z","iopub.status.idle":"2025-12-20T10:45:29.056326Z","shell.execute_reply.started":"2025-12-20T10:45:14.905788Z","shell.execute_reply":"2025-12-20T10:45:29.055746Z"}},"outputs":[{"name":"stdout","text":"Total vocab size: 31572\nTrain samples: (1773893, 59) (1773893,)\nMax sequence length: 60\nValidation samples: (444740, 59) (444740,)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GRU, Dense, BatchNormalization, Dropout\nfrom tensorflow.keras import regularizers\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size_ng3, 512, input_length=maxlen_ng3-1))\nmodel.add(GRU(512, return_sequences=True, dropout=0.2, recurrent_dropout=0.1))\nmodel.add(GRU(512, return_sequences=False, dropout=0.25))\n\nmodel.add(Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Dense(vocab_size_ng3, activation=\"softmax\"))\n\nmodel.compile(\n    loss=\"sparse_categorical_crossentropy\",\n    optimizer=Adam(learning_rate=0.0003, clipnorm=1.0),\n    metrics=[\"accuracy\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T10:48:23.955596Z","iopub.execute_input":"2025-12-20T10:48:23.956406Z","iopub.status.idle":"2025-12-20T10:48:23.982403Z","shell.execute_reply.started":"2025-12-20T10:48:23.956380Z","shell.execute_reply":"2025-12-20T10:48:23.981473Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model.build((None, maxlen_ng3-1))\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T10:48:39.159600Z","iopub.execute_input":"2025-12-20T10:48:39.160265Z","iopub.status.idle":"2025-12-20T10:48:39.179048Z","shell.execute_reply.started":"2025-12-20T10:48:39.160238Z","shell.execute_reply":"2025-12-20T10:48:39.178490Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │    \u001b[38;5;34m16,164,864\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ gru_4 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,575,936\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ gru_5 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,575,936\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31572\u001b[0m)          │     \u001b[38;5;34m8,114,004\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">16,164,864</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ gru_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,575,936</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ gru_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,575,936</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31572</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,114,004</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,824,724\u001b[0m (106.14 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,824,724</span> (106.14 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m27,824,724\u001b[0m (106.14 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,824,724</span> (106.14 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nearly_stop_ng = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\nlr_scheduler_ng = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1)\n\nhistory_ng = model.fit(\n    x0_ng, y0_ng,\n    epochs=30,\n    batch_size=128,\n    validation_data=(xv_ng, yv_ng),\n    callbacks=[early_stop_ng, lr_scheduler_ng]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T14:31:06.434361Z","iopub.execute_input":"2025-12-20T14:31:06.434559Z","iopub.status.idle":"2025-12-20T14:31:06.501504Z","shell.execute_reply.started":"2025-12-20T14:31:06.434541Z","shell.execute_reply":"2025-12-20T14:31:06.500530Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_38/2816982840.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mearly_stop_ng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlr_scheduler_ng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m history_ng = model.fit(\n\u001b[1;32m      5\u001b[0m     \u001b[0mx0_ng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0_ng\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'EarlyStopping' is not defined"],"ename":"NameError","evalue":"name 'EarlyStopping' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# path to your model file (use quotes and correct directory)\nmodel_path = \"/kaggle/input/gru-model/tensorflow2/default/1/gru_mode.h5\"\n\n# load the model\nmodel1 = load_model(model_path)\n\n# verify load\nmodel1.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_loss, final_acc = model.evaluate(xv_ng, yv_ng, batch_size=128, verbose=1)\n\nprint(\"Final Validation Loss:\", final_loss)\nprint(\"Final Validation Accuracy:\", final_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T17:44:03.184866Z","iopub.execute_input":"2025-12-19T17:44:03.185534Z","iopub.status.idle":"2025-12-19T17:46:55.488369Z","shell.execute_reply.started":"2025-12-19T17:44:03.185509Z","shell.execute_reply":"2025-12-19T17:46:55.487651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nprint(\"Perplexity:\", np.exp(model.evaluate(xv_ng, yv_ng)[0]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T19:32:02.648169Z","iopub.execute_input":"2025-12-19T19:32:02.648509Z","iopub.status.idle":"2025-12-19T19:32:07.253799Z","shell.execute_reply.started":"2025-12-19T19:32:02.648488Z","shell.execute_reply":"2025-12-19T19:32:07.252814Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m  112/13899\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:56\u001b[0m 39ms/step - accuracy: 0.2698 - loss: 4.3563","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_38/898312771.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Perplexity:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxv_ng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myv_ng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_evaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":31},{"cell_type":"code","source":"model.save(\"predict_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T19:37:12.970393Z","iopub.execute_input":"2025-12-19T19:37:12.970645Z","iopub.status.idle":"2025-12-19T19:37:13.479590Z","shell.execute_reply.started":"2025-12-19T19:37:12.970627Z","shell.execute_reply":"2025-12-19T19:37:13.479042Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"import pickle\nwith open(\"tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer3, f)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport pickle\n\n\nwith open(\"tokenizer.pkl\", \"rb\") as f:\n    tok3 = pickle.load(f)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# path to your model file (use quotes and correct directory)\nmodel_path = \"/kaggle/input/gru-model/tensorflow2/default/1/gru_mode.h5\"\n\n# load the model\nmodel1 = load_model(model_path)\n\n# verify load\nmodel1.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T15:54:47.017944Z","iopub.execute_input":"2025-11-25T15:54:47.018438Z","iopub.status.idle":"2025-11-25T15:54:47.758279Z","shell.execute_reply.started":"2025-11-25T15:54:47.018415Z","shell.execute_reply":"2025-11-25T15:54:47.757523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def output(seed_text, model, max_len, tokenizer, next_words=20):\n    in_text = seed_text\n\n    for _ in range(next_words):\n        seq = tokenizer.texts_to_sequences([in_text])[0]  # fixed typo\n        seq = seq[-(max_len-1):]  # keep last max_len-1 tokens\n        seq = np.pad(seq, (max_len-1-len(seq), 0), \"constant\")\n\n        y_pred = model.predict(seq.reshape(1, max_len-1), verbose=0)  # fixed typo\n        next_index = np.argmax(y_pred)\n        next_word = tokenizer.index_word[next_index]\n\n        in_text += \" \" + next_word\n\n    return in_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T19:32:12.450222Z","iopub.execute_input":"2025-12-19T19:32:12.450487Z","iopub.status.idle":"2025-12-19T19:32:12.455447Z","shell.execute_reply.started":"2025-12-19T19:32:12.450467Z","shell.execute_reply":"2025-12-19T19:32:12.454784Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import numpy as np\n\ndef top_k_temperature_sampling(preds, k=40, temperature=0.8):\n    preds = np.asarray(preds).astype(\"float64\")\n\n    # temperature scaling\n    preds = np.log(preds + 1e-9) / temperature\n\n    # keep only top-k probabilities\n    top_k_indices = np.argpartition(preds, -k)[-k:]\n    top_k_logits = preds[top_k_indices]\n\n    # softmax\n    exp_preds = np.exp(top_k_logits)\n    probs = exp_preds / np.sum(exp_preds)\n\n    # sample\n    return np.random.choice(top_k_indices, p=probs)\n\n\ndef output(\n    seed_text,\n    model,\n    max_len,\n    tokenizer,\n    next_words=20,\n    temperature=0.8,\n    top_k=40\n):\n    in_text = seed_text\n\n    for _ in range(next_words):\n        seq = tokenizer.texts_to_sequences([in_text])[0]\n        seq = seq[-(max_len - 1):]\n        seq = np.pad(seq, (max_len - 1 - len(seq), 0), \"constant\")\n\n        preds = model.predict(seq.reshape(1, max_len - 1), verbose=0)[0]\n\n        next_index = top_k_temperature_sampling(\n            preds,\n            k=top_k,\n            temperature=temperature\n        )\n\n        next_word = tokenizer.index_word.get(next_index, \"\")\n        in_text += \" \" + next_word\n\n    return in_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T19:41:11.213801Z","iopub.execute_input":"2025-12-19T19:41:11.214379Z","iopub.status.idle":"2025-12-19T19:41:11.220793Z","shell.execute_reply.started":"2025-12-19T19:41:11.214355Z","shell.execute_reply":"2025-12-19T19:41:11.220028Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"output(\n    seed_text=\"i love\",\n    model=model,\n    max_len=maxlen_ng3,\n    tokenizer=tokenizer_ng3,\n    next_words=5,\n    temperature=0.8,\n    top_k=10\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T19:47:52.780960Z","iopub.execute_input":"2025-12-19T19:47:52.781255Z","iopub.status.idle":"2025-12-19T19:47:53.267058Z","shell.execute_reply.started":"2025-12-19T19:47:52.781235Z","shell.execute_reply":"2025-12-19T19:47:53.266390Z"}},"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"'i love you all right now i'"},"metadata":{}}],"execution_count":82},{"cell_type":"code","source":"print(output(\n    seed_text=\"i love\",\n    next_words=60,\n    model=model,\n    max_len=maxlen_ng3,\n    tokenizer=tokenizer_ng3,\n   \n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T19:38:14.127621Z","iopub.execute_input":"2025-12-19T19:38:14.128324Z","iopub.status.idle":"2025-12-19T19:38:19.965272Z","shell.execute_reply.started":"2025-12-19T19:38:14.128299Z","shell.execute_reply":"2025-12-19T19:38:19.964549Z"}},"outputs":[{"name":"stdout","text":"i love you too much to you and i don't know what i want to do to you the way you know that i was a little bit of a little bit of a little bit of the same and i was a little bit of a little bit of a little bit of a little bit of the same to be\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"import numpy as np\nprint(\"Perplexity:\", np.exp(model1.evaluate(X_val1, y_val1)[0]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T15:56:05.858198Z","iopub.execute_input":"2025-11-25T15:56:05.858741Z","iopub.status.idle":"2025-11-25T15:58:27.286061Z","shell.execute_reply.started":"2025-11-25T15:56:05.858718Z","shell.execute_reply":"2025-11-25T15:58:27.285298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GRU, Dense, BatchNormalization, Dropout\nfrom tensorflow.keras import regularizers\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# -----------------------------------------------------\n# LOAD DATA\n# -----------------------------------------------------\nsentences_small = []\n\nwith open(\"/kaggle/input/subtitles-dataset/en (1).txt\", \"r\", encoding=\"utf-8\") as f:\n    for i, line in enumerate(f):\n        if i >= 100000:\n            break\n        line = line.strip()\n        if line:\n            sentences_small.append(line.lower())\n\ntrain_sentences5, val_sentences5 = train_test_split(sentences_small, test_size=0.2, random_state=42)\nprint(\"Train sentences:\", len(train_sentences5))\nprint(\"Validation sentences:\", len(val_sentences5))\n\n# -----------------------------------------------------\n# TOKENIZER (new tokenizer for this model)\n# -----------------------------------------------------\ntokenizer5 = Tokenizer(oov_token=\"<OOV>\")\ntokenizer5.fit_on_texts(train_sentences5)\n\ntotal_words_5 = len(tokenizer5.word_index) + 1\nprint(\"Total vocab size:\", total_words_5)\n\n# -----------------------------------------------------\n# N-GRAM SEQUENCE BUILDER\n# -----------------------------------------------------\ndef build_ngram_sequences(lines, tokenizer, max_len=None):\n    sequences = []\n    for line in lines:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram = token_list[:i+1]\n            sequences.append(n_gram)\n\n    if not sequences:\n        return None, None, max_len\n\n    # Calculate max_len automatically OR use given\n    if max_len is None:\n        max_len = max(len(x) for x in sequences)\n\n    sequences = pad_sequences(sequences, maxlen=max_len, padding=\"pre\")\n    X, y = sequences[:, :-1], sequences[:, -1]\n    return X, y, max_len\n\n# -----------------------------------------------------\n# BUILD TRAIN SEQUENCES\n# -----------------------------------------------------\nX5, y5, max_len_seq5 = build_ngram_sequences(train_sentences5, tokenizer5)\nprint(\"Train samples:\", X5.shape, y5.shape)\nprint(\"Max sequence length:\", max_len_seq5)\n\n# -----------------------------------------------------\n# VALIDATION SEQUENCES (use same max_len)\n# -----------------------------------------------------\nX_val5, y_val5, _ = build_ngram_sequences(val_sentences5, tokenizer5, max_len=max_len_seq5)\nprint(\"Validation samples:\", X_val5.shape, y_val5.shape)\n\n# -----------------------------------------------------\n# MODEL\n# -----------------------------------------------------\nmodel = Sequential()\nmodel.add(Embedding(total_words_5, 128, input_length=max_len_seq5-1))\nmodel.add(GRU(128, return_sequences=True, dropout=0.1))\nmodel.add(GRU(128, dropout=0.3))\n\nmodel.add(Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(total_words_5, activation=\"softmax\"))\n\nfrom tensorflow.keras.optimizers import Adam\nopt = Adam(learning_rate=0.001, clipnorm=1.0)\n\nmodel.compile(\n    loss=\"sparse_categorical_crossentropy\",\n    optimizer=opt,\n    metrics=[\"accuracy\"]\n)\n\n\nmodel.summary()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T15:13:33.601938Z","iopub.execute_input":"2025-11-25T15:13:33.602178Z","iopub.status.idle":"2025-11-25T15:13:36.575939Z","shell.execute_reply.started":"2025-11-25T15:13:33.602162Z","shell.execute_reply":"2025-11-25T15:13:36.575317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nprint(\"Any empty rows in X5?  →\", np.any(np.all(X5 == 0, axis=1)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T14:19:41.515043Z","iopub.execute_input":"2025-11-25T14:19:41.515305Z","iopub.status.idle":"2025-11-25T14:19:41.550756Z","shell.execute_reply.started":"2025-11-25T14:19:41.515285Z","shell.execute_reply":"2025-11-25T14:19:41.550147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Any invalid y labels? →\", np.any((y5 == 0) | (y5 >= total_words_5)))\nprint(\"Min y:\", y5.min(), \"Max y:\", y5.max(), \"Vocab:\", total_words_5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T14:20:25.486694Z","iopub.execute_input":"2025-11-25T14:20:25.486968Z","iopub.status.idle":"2025-11-25T14:20:25.503965Z","shell.execute_reply.started":"2025-11-25T14:20:25.486947Z","shell.execute_reply":"2025-11-25T14:20:25.503216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nearly_stop = EarlyStopping(\n    monitor=\"val_loss\",\n    patience=3,\n    restore_best_weights=True\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=2,\n    verbose=1\n)\n\nhistory = model.fit(\n    X5, y5,\n    epochs=30,\n    batch_size=64,\n    validation_data=(X_val5, y_val5),\n    callbacks=[early_stop, lr_scheduler]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T15:13:38.937830Z","iopub.execute_input":"2025-11-25T15:13:38.938096Z","iopub.status.idle":"2025-11-25T15:50:38.669120Z","shell.execute_reply.started":"2025-11-25T15:13:38.938075Z","shell.execute_reply":"2025-11-25T15:50:38.668455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nprint(\"Perplexity:\", np.exp(model.evaluate(X_val5, y_val5)[0]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T15:52:36.419359Z","iopub.execute_input":"2025-11-25T15:52:36.419666Z","iopub.status.idle":"2025-11-25T15:52:52.240746Z","shell.execute_reply.started":"2025-11-25T15:52:36.419646Z","shell.execute_reply":"2025-11-25T15:52:52.240155Z"}},"outputs":[],"execution_count":null}]}